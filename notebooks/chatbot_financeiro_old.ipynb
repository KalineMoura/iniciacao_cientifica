{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Instala√ß√£o de dependencias"
      ],
      "metadata": {
        "id": "wGob0g7Jopmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit transformers torch langchain langchain-community faiss-cpu sentence-transformers huggingface-hub python-dotenv pyngrok\n"
      ],
      "metadata": {
        "id": "qoS-PfvncmGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Token do Hugging Face"
      ],
      "metadata": {
        "id": "ZL9OYJ7qp9mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "HUGGINGFACEHUB_API_TOKEN=\"token\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgoLdPlip3PQ",
        "outputId": "24ef5c2a-bd43-4534-a3cd-cbb0086229a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Carregamento + responder"
      ],
      "metadata": {
        "id": "JUxRx3r3qWIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import streamlit as st\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Carrega vari√°veis do .env\n",
        "load_dotenv()\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# Carrega markdown j√° chunkado\n",
        "loader = TextLoader(\"/content/chunks_exemplos.md\")\n",
        "docs = loader.load()\n",
        "splits = docs\n",
        "\n",
        "# Indexa√ß√£o com embeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 4})\n",
        "\n",
        "# Carrega modelo local phi-4-mini\n",
        "phi_model_id = \"microsoft/phi-4-mini-instruct\"\n",
        "phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_id)\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    phi_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Inicializa hist√≥rico no session_state\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "# Fun√ß√£o que responde perguntas\n",
        "def responder(pergunta):\n",
        "    try:\n",
        "        if not st.session_state.chat_history:\n",
        "            st.session_state.chat_history.append(\n",
        "                AIMessage(content=\"Ol√°! Me envie sua d√∫vida.\")\n",
        "            )\n",
        "\n",
        "        docs_relevantes = retriever.get_relevant_documents(pergunta)\n",
        "        contexto = \"\\n\\n\".join([doc.page_content[:1000] for doc in docs_relevantes])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Voc√™ √© um assistente financeiro. Com base no seguinte contexto, responda √† pergunta do usu√°rio de forma clara e objetiva. Se n√£o souber, diga que n√£o sabe.\n",
        "\n",
        "        Contexto:\n",
        "        {contexto}\n",
        "\n",
        "        Pergunta:\n",
        "        {pergunta}\n",
        "        \"\"\"\n",
        "\n",
        "        input_ids = phi_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(phi_model.device)\n",
        "        output = phi_model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.1,\n",
        "            do_sample=False\n",
        "        )[0]\n",
        "\n",
        "        full_text = phi_tokenizer.decode(output, skip_special_tokens=True)\n",
        "        resposta = full_text[len(prompt):].strip()\n",
        "\n",
        "        st.session_state.chat_history.append(HumanMessage(content=pergunta))\n",
        "        st.session_state.chat_history.append(AIMessage(content=resposta))\n",
        "\n",
        "        placeholder = st.empty()\n",
        "        buffer = \"\"\n",
        "        for char in resposta:\n",
        "            buffer += char\n",
        "            placeholder.markdown(buffer)\n",
        "            time.sleep(0.015)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Erro: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBIWZIOnqftw",
        "outputId": "cf64cdeb-a888-4463-a8d3-db185e4e2427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar app.py (interface Streamlit)"
      ],
      "metadata": {
        "id": "YWhyz8iiqllg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from main import responder\n",
        "\n",
        "st.set_page_config(page_title=\"Seu Assistente Financeiro\", page_icon=\"üß†\")\n",
        "st.title(\"üß† Seu Assistente Financeiro\")\n",
        "st.markdown(\"Converse com seu assistente baseado em conhecimento financeiro!\")\n",
        "\n",
        "# Inicializa o hist√≥rico\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "# üîò Bot√£o para limpar hist√≥rico\n",
        "if st.button(\"üóëÔ∏è Limpar hist√≥rico da conversa\"):\n",
        "    st.session_state.chat_history = []\n",
        "    st.experimental_rerun()\n",
        "\n",
        "# Campo de entrada\n",
        "pergunta = st.chat_input(\"Digite sua pergunta:\")\n",
        "\n",
        "# Exibe hist√≥rico\n",
        "for msg in st.session_state.chat_history:\n",
        "    with st.chat_message(\"user\" if isinstance(msg, HumanMessage) else \"assistant\"):\n",
        "        st.markdown(msg.content)\n",
        "\n",
        "# Gera resposta\n",
        "if pergunta:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(pergunta)\n",
        "\n",
        "    responder(pergunta)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        pass  # A resposta j√° foi exibida na fun√ß√£o responder\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pc3wPpXqlJ1",
        "outputId": "91d0d086-cb41-47f0-c07d-3b14e05b2edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    }
  ]
}