########################################################
import torch, bitsandbytes as bnb, accelerate
print("torch:", torch.__version__,
      "| bnb:", bnb.__version__,
      "| accelerate:", accelerate.__version__,
      "| CUDA?", torch.cuda.is_available())
########################################################
# src/app.py
import streamlit as st
import time
from langchain_core.messages import AIMessage, HumanMessage
from main import gerar_resposta  # chama a funÃ§Ã£o lazyâ€‘cacheada

st.set_page_config(page_title="Assistente Financeiro", page_icon="ğŸ’°")
st.title("ğŸ’° Assistente Financeiro")

# ---- estado da conversa ----------------------------------------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content="OlÃ¡! Como posso ajudar nas suas finanÃ§as hoje?")
    ]

if st.button("ğŸ—‘ï¸ Limpar conversa"):
    st.session_state.chat_history = st.session_state.chat_history[:1]
    st.experimental_rerun()

# ---- exibe histÃ³rico existente --------------------------------------------
for msg in st.session_state.chat_history:
    role = "user" if isinstance(msg, HumanMessage) else "assistant"
    with st.chat_message(role):
        st.markdown(msg.content)

# ---- input do usuÃ¡rio ------------------------------------------------------
pergunta = st.chat_input("Digite sua perguntaâ€¦")

if pergunta:
    st.session_state.chat_history.append(HumanMessage(content=pergunta))

    with st.chat_message("assistant"):
        placeholder = st.empty()
        with st.spinner("Gerando resposta..."):
            try:
                resposta = gerar_resposta(pergunta)
            except Exception as e:
                resposta = f"Desculpe, ocorreu um erro: {e}"

        # animaÃ§Ã£o de â€œdigitandoâ€¦â€
        buf = ""
        for ch in resposta:
            buf += ch
            placeholder.markdown(buf)
            time.sleep(0.012)

        st.session_state.chat_history.append(AIMessage(content=resposta))
